{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzPq/bO8r9ZWyKWai9ZicG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qoyyimil/pba/blob/main/notebooks/IndoNLU_SMSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okai-q6HT23P"
      },
      "outputs": [],
      "source": [
        "!pip install nltk spacy PySastrawi datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datasets<3.0.0\" -U\n",
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "id": "LPXqADxDUNAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# dataset IndoNLU SMSA\n",
        "dataset = load_dataset(\n",
        "    \"indonlp/indonlu\",\n",
        "    name=\"smsa\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Konversi ke pandas untuk eksplorasi\n",
        "import pandas as pd\n",
        "df = dataset.to_pandas()\n",
        "\n",
        "# Pilih beberapa contoh teks\n",
        "sample_texts = df['text'][:3].tolist()  # Ambil 3 teks pertama\n",
        "labels = df['label'][:3].tolist()  # Ambil label sentimen\n",
        "\n",
        "# Inisialisasi stemmer Sastrawi\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "# Tokenisasi dan analisis untuk setiap teks\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nContoh Teks {i+1} (Label: {labels[i]}):\")\n",
        "    print(\"Teks Asli:\", text)\n",
        "\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"Token:\", tokens)\n",
        "    print(\"Jumlah Token:\", len(tokens))\n",
        "    print(\"Token Unik:\", len(set(tokens)))\n",
        "\n",
        "    # Stemming (opsional untuk analisis)\n",
        "    stemmed_text = stemmer.stem(text)\n",
        "    stemmed_tokens = word_tokenize(stemmed_text)\n",
        "    print(\"Token setelah Stemming:\", stemmed_tokens)\n",
        "    print(\"Jumlah Token setelah Stemming:\", len(stemmed_tokens))\n",
        "\n",
        "    # Simpan token ke file\n",
        "    with open(f'tokens_sample_{i+1}.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(tokens))\n",
        "    print(f\"Token disimpan ke tokens_sample_{i+1}.txt\")\n",
        "\n",
        "# Analisis tambahan: Frekuensi kata\n",
        "from collections import Counter\n",
        "all_tokens = []\n",
        "for text in sample_texts:\n",
        "    all_tokens.extend(word_tokenize(text.lower()))\n",
        "word_freq = Counter(all_tokens).most_common(10)\n",
        "print(\"\\n10 Kata Teratas:\", word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdJS5xjlURWw",
        "outputId": "02ed6349-0f32-4fa7-ce3b-9248ad90d71e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contoh Teks 1 (Label: 0):\n",
            "Teks Asli: warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\n",
            "Token: ['warung', 'ini', 'dimiliki', 'oleh', 'pengusaha', 'pabrik', 'tahu', 'yang', 'sudah', 'puluhan', 'tahun', 'terkenal', 'membuat', 'tahu', 'putih', 'di', 'bandung', '.', 'tahu', 'berkualitas', ',', 'dipadu', 'keahlian', 'memasak', ',', 'dipadu', 'kretivitas', ',', 'jadilah', 'warung', 'yang', 'menyajikan', 'menu', 'utama', 'berbahan', 'tahu', ',', 'ditambah', 'menu', 'umum', 'lain', 'seperti', 'ayam', '.', 'semuanya', 'selera', 'indonesia', '.', 'harga', 'cukup', 'terjangkau', '.', 'jangan', 'lewatkan', 'tahu', 'bletoka', 'nya', ',', 'tidak', 'kalah', 'dengan', 'yang', 'asli', 'dari', 'tegal', '!']\n",
            "Jumlah Token: 66\n",
            "Token Unik: 50\n",
            "Token setelah Stemming: ['warung', 'ini', 'milik', 'oleh', 'usaha', 'pabrik', 'tahu', 'yang', 'sudah', 'puluh', 'tahun', 'kenal', 'buat', 'tahu', 'putih', 'di', 'bandung', 'tahu', 'kualitas', 'padu', 'ahli', 'masak', 'padu', 'kretivitas', 'jadi', 'warung', 'yang', 'saji', 'menu', 'utama', 'bahan', 'tahu', 'tambah', 'menu', 'umum', 'lain', 'seperti', 'ayam', 'semua', 'selera', 'indonesia', 'harga', 'cukup', 'jangkau', 'jangan', 'lewat', 'tahu', 'bletoka', 'nya', 'tidak', 'kalah', 'dengan', 'yang', 'asli', 'dari', 'tegal']\n",
            "Jumlah Token setelah Stemming: 56\n",
            "Token disimpan ke tokens_sample_1.txt\n",
            "\n",
            "Contoh Teks 2 (Label: 1):\n",
            "Teks Asli: mohon ulama lurus dan k212 mmbri hujjah partai apa yang harus diwlh agar suara islam tidak pecah-pecah\n",
            "Token: ['mohon', 'ulama', 'lurus', 'dan', 'k212', 'mmbri', 'hujjah', 'partai', 'apa', 'yang', 'harus', 'diwlh', 'agar', 'suara', 'islam', 'tidak', 'pecah-pecah']\n",
            "Jumlah Token: 17\n",
            "Token Unik: 17\n",
            "Token setelah Stemming: ['mohon', 'ulama', 'lurus', 'dan', 'k212', 'mmbri', 'hujjah', 'partai', 'apa', 'yang', 'harus', 'diwlh', 'agar', 'suara', 'islam', 'tidak', 'pecah']\n",
            "Jumlah Token setelah Stemming: 17\n",
            "Token disimpan ke tokens_sample_2.txt\n",
            "\n",
            "Contoh Teks 3 (Label: 0):\n",
            "Teks Asli: lokasi strategis di jalan sumatera bandung . tempat nya nyaman terutama sofa di lantai 2 . paella nya enak , sangat pas dimakan dengan minum bir dingin . appetiser nya juga enak-enak .\n",
            "Token: ['lokasi', 'strategis', 'di', 'jalan', 'sumatera', 'bandung', '.', 'tempat', 'nya', 'nyaman', 'terutama', 'sofa', 'di', 'lantai', '2', '.', 'paella', 'nya', 'enak', ',', 'sangat', 'pas', 'dimakan', 'dengan', 'minum', 'bir', 'dingin', '.', 'appetiser', 'nya', 'juga', 'enak-enak', '.']\n",
            "Jumlah Token: 33\n",
            "Token Unik: 27\n",
            "Token setelah Stemming: ['lokasi', 'strategis', 'di', 'jalan', 'sumatera', 'bandung', 'tempat', 'nya', 'nyaman', 'utama', 'sofa', 'di', 'lantai', '2', 'paella', 'nya', 'enak', 'sangat', 'pas', 'makan', 'dengan', 'minum', 'bir', 'dingin', 'appetiser', 'nya', 'juga', 'enak']\n",
            "Jumlah Token setelah Stemming: 28\n",
            "Token disimpan ke tokens_sample_3.txt\n",
            "\n",
            "10 Kata Teratas: [('.', 8), (',', 6), ('tahu', 5), ('yang', 4), ('nya', 4), ('di', 3), ('warung', 2), ('bandung', 2), ('dipadu', 2), ('menu', 2)]\n"
          ]
        }
      ]
    }
  ]
}